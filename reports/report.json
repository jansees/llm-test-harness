{"created": 1756662890.6605716, "duration": 9.48472547531128, "exitcode": 1, "root": "C:\\GitHubGalore\\Testsuite1\\llm-test-harness", "environment": {}, "summary": {"failed": 1, "total": 1, "collected": 4, "deselected": 3}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "harness", "type": "Dir"}]}, {"nodeid": "harness/test_determinism.py", "outcome": "passed", "result": [{"nodeid": "harness/test_determinism.py::test_determinism", "type": "Function", "lineno": 0, "deselected": true}]}, {"nodeid": "harness/test_functional_and_safety.py", "outcome": "passed", "result": [{"nodeid": "harness/test_functional_and_safety.py::test_functional_golden_set", "type": "Function", "lineno": 4, "deselected": true}, {"nodeid": "harness/test_functional_and_safety.py::test_basic_safety", "type": "Function", "lineno": 25, "deselected": true}]}, {"nodeid": "harness/test_latency_throughput.py", "outcome": "passed", "result": [{"nodeid": "harness/test_latency_throughput.py::test_latency_throughput", "type": "Function", "lineno": 3}]}, {"nodeid": "harness", "outcome": "passed", "result": [{"nodeid": "harness/test_determinism.py", "type": "Module"}, {"nodeid": "harness/test_functional_and_safety.py", "type": "Module"}, {"nodeid": "harness/test_latency_throughput.py", "type": "Module"}]}], "tests": [{"nodeid": "harness/test_latency_throughput.py::test_latency_throughput", "lineno": 3, "outcome": "failed", "keywords": ["test_latency_throughput", "test_latency_throughput.py", "harness", "llm-test-harness", ""], "setup": {"duration": 0.0005089999758638442, "outcome": "passed"}, "call": {"duration": 9.33312649995787, "outcome": "failed", "crash": {"path": "C:\\GitHubGalore\\Testsuite1\\llm-test-harness\\harness\\test_latency_throughput.py", "lineno": 14, "message": "AssertionError: throughput too low for tiny model\nassert 2.7229703780140655 >= 3.0\n +  where 2.7229703780140655 = <function mean at 0x000002151E6625C0>([1.8908007273998033, 3.2053585903316315, 3.072751816310761])\n +    where <function mean at 0x000002151E6625C0> = stats.mean"}, "traceback": [{"path": "harness\\test_latency_throughput.py", "lineno": 14, "message": "AssertionError"}], "stdout": "TIME  4.231011700001545\nTEXT  \nSay 'hello' exactly.\nUSAGE  {'completion_tokens': 8, 'prompt_tokens': 48, 'total_tokens': 56}\nTIME  2.4958205999573693\nTEXT  \nSay 'hello' exactly.\nUSAGE  {'completion_tokens': 8, 'prompt_tokens': 48, 'total_tokens': 56}\nTIME  2.6035295000183396\nTEXT  \nSay 'hello' exactly.\nUSAGE  {'completion_tokens': 8, 'prompt_tokens': 48, 'total_tokens': 56}\n", "longrepr": "client = <harness.client.LlamaCppClient object at 0x000002151C24D050>\n\n    def test_latency_throughput(client):\n        lats, tps = [], []\n        for _ in range(3):\n            out, dt, usage = client.chat([{\"role\":\"user\",\"content\":\"Say 'hello' exactly.\"}],\n                                         temperature=0.0, seed=42, max_tokens=8)\n            assert \"hello\" in out.lower()\n            lats.append(dt)\n            tps.append(tokens_per_second(usage, dt))\n        p95 = sorted(lats)[int(0.95*len(lats))-1]\n        assert p95 < 5.0, f\"p95 latency too high: {p95:.2f}s\"\n>       assert stats.mean(tps) >= 3.0, \"throughput too low for tiny model\"\nE       AssertionError: throughput too low for tiny model\nE       assert 2.7229703780140655 >= 3.0\nE        +  where 2.7229703780140655 = <function mean at 0x000002151E6625C0>([1.8908007273998033, 3.2053585903316315, 3.072751816310761])\nE        +    where <function mean at 0x000002151E6625C0> = stats.mean\n\nharness\\test_latency_throughput.py:14: AssertionError"}, "teardown": {"duration": 0.00044050003634765744, "outcome": "passed"}}]}